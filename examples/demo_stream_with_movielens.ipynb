{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recorded Demo of RecNextEval for SIGIR 2026 Presentation\n",
    "\n",
    "Paper title: RecNextEval: A Reference Implementation for Temporal Next-Batch Recommendation Evaluation\n",
    "\n",
    "Presenter: Ng Tze Kean\n",
    "\n",
    "Presentation paramters:\n",
    "\n",
    "- Dataset: MovieLens100K\n",
    "- Top K = 10\n",
    "- Algorithm: ItemKNNIncremental, RecentPopularity\n",
    "- First timestamp split at epoch time 875_156_710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "epoch_ts = 875_156_710\n",
    "converted_dt = datetime.fromtimestamp(epoch_ts, tz=timezone.utc)\n",
    "converted_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recnexteval.datasets import MovieLens100K\n",
    "from recnexteval.settings import SlidingWindowSetting\n",
    "\n",
    "\n",
    "k = 10\n",
    "dataset = MovieLens100K()\n",
    "data = dataset.load()\n",
    "\n",
    "setting_window = SlidingWindowSetting(\n",
    "    training_t=875_156_710,\n",
    "    window_size=60 * 60 * 24 * 30,  # day times N\n",
    "    top_K=k\n",
    ")\n",
    "\n",
    "setting_window.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluator instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recnexteval.evaluators import EvaluatorStreamerBuilder\n",
    "\n",
    "\n",
    "builder = EvaluatorStreamerBuilder(\n",
    "    ignore_unknown_item=False,\n",
    "    ignore_unknown_user=False,\n",
    ")\n",
    "builder.add_setting(setting=setting_window)\n",
    "builder.set_metric_k(k)\n",
    "builder.add_metric(\"HitK\")\n",
    "builder.add_metric(\"NDCGK\")\n",
    "evaluator = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recnexteval.algorithms import ItemKNNIncremental, RecentPopularity\n",
    "\n",
    "\n",
    "external_model = [ItemKNNIncremental(K=k), RecentPopularity(K=k)]\n",
    "ids = []\n",
    "for model in external_model:\n",
    "    ids.append(evaluator.register_model(algorithm=model))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.start_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data release phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_id in zip(external_model, ids):\n",
    "    training_data = evaluator.get_training_data(model_id)\n",
    "    if training_data is None:\n",
    "        raise ValueError(\"No data available for the external model.\")\n",
    "    model.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_id in zip(external_model, ids):\n",
    "    unlabeled_data = evaluator.get_unlabeled_data(model_id)\n",
    "    prediction = model.predict(unlabeled_data)\n",
    "    evaluator.submit_prediction(model_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through windows\n",
    "\n",
    "Evaluator iterates through time windows, releasing data and collecting predictions at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(setting_window.num_split - 1):\n",
    "    for model, model_id in zip(external_model, ids):\n",
    "        training_data = evaluator.get_training_data(model_id)\n",
    "        model.fit(training_data)\n",
    "        unlabeled_data = evaluator.get_unlabeled_data(model_id)\n",
    "        prediction = model.predict(unlabeled_data)\n",
    "        evaluator.submit_prediction(model_id, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.get_all_algorithm_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.metric_results(\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.metric_results(\"micro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.metric_results(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.metric_results(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.plot_window_level_metric()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recnexteval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
